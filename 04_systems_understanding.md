# Phase 4: Systems Understanding

**Sources:** Whitney et al. (2018), Methods Guide draft, Luu et al. (2024)

Building a comprehensive understanding of the system is essential before quantitative modeling can begin. This phase covers stakeholder identification, expert selection, knowledge elicitation, and developing impact pathway models.

---

## Purpose

Establish a shared, comprehensive understanding of the decision context by:
- Identifying who is affected by and can inform the decision
- Selecting appropriate experts to contribute to model development
- Eliciting knowledge systematically to capture the system's structure
- Developing impact pathway models that represent how decisions lead to outcomes

---

## 4.1 Establish Current Systems Understanding

### Checklist

- [ ] **Conduct literature review**
  - Historical context of the decision area
  - Current trends and emerging challenges
  - Existing models and frameworks
  
- [ ] **Direct observation**
  - Site visits where feasible
  - Firsthand perspectives not captured in literature
  
- [ ] **Stakeholder interviews**
  - Preliminary conversations with key informants
  - Qualitative insights into system dynamics
  
- [ ] **Synthesize information**
  - Document what is known and unknown
  - Identify key uncertainties and knowledge gaps

### Outputs

- Systems context document
- Preliminary list of key variables and relationships
- Knowledge gap assessment

---

## 4.2 Stakeholder Identification

### Who is a Stakeholder?

Stakeholders are individuals, groups, or entities who:
- Are directly or indirectly affected by the decision
- Can affect or be affected by decision outcomes
- Hold a vested interest in the process or results

**Examples in agricultural decisions:**
- Farmers and farming communities
- Government agencies and policymakers
- Regulatory bodies
- Local businesses and value chain actors
- Investors and development organizations
- Researchers and academics
- NGOs and civil society

### Stakeholder Identification Process

Following Luu et al. (2024):

1. **Generate initial pool**
   - Desk review to identify potential stakeholders
   - Talk to collaborators and decision-makers
   
2. **Assess key attributes**
   
   | Attribute | Description |
   |-----------|-------------|
   | Availability | Time and commitment to participate |
   | Experience | Knowledge within the decision context |
   | Gender | Ensure gender-balanced representation |
   | Expertise | Specialized technical knowledge |
   | Interest | Stake in the outcome |
   | Influence | Power to affect the decision |
   | Relevance | Direct connection to the problem |
   | Attitude | Openness to collaboration |

3. **Score and categorize**
   - Score each attribute (e.g., 0–5 scale)
   - Use scores to prioritize engagement

### Stakeholder vs. Expert

| Stakeholder | Expert |
|-------------|--------|
| Affected by decision outcomes | Possesses decision-relevant knowledge |
| Contributes values and preferences | Contributes technical information |
| May or may not have technical expertise | May or may not be directly affected |
| Essential for understanding impacts | Essential for building models |

*Note: Individuals can be both stakeholders AND experts.*

### Checklist

- [ ] Desk review completed — initial stakeholder list generated
- [ ] Decision-maker consulted — stakeholder list refined
- [ ] Key attributes assessed for each stakeholder
- [ ] Stakeholders categorized by engagement level
- [ ] Gender balance verified
- [ ] Engagement strategy defined for each category

---

## 4.3 Expert Identification

### Who is an Expert?

An expert is someone who possesses:
- Decision-relevant knowledge
- Experience in the specific domain
- Ability to make informed estimates about uncertainties

**Expert sources:**
- Academia (nutritionists, agronomists, economists)
- Government institutions
- Local communities and practitioners
- Development organizations
- Private sector specialists

### Expert Selection Process

1. **Define knowledge domains needed**
   - What aspects of the system require expert input?
   - What uncertainties need to be quantified?
   
2. **Identify candidates**
   - Professional networks and collaborations
   - Conference and workshop participants
   - Recommendations from stakeholders
   
3. **Assess expertise**
   
   | Criterion | Question |
   |-----------|----------|
   | Domain knowledge | Do they understand this specific system? |
   | Track record | Have they contributed to similar efforts? |
   | Data access | Do they have relevant data or observations? |
   | Availability | Can they commit time to the process? |
   | Communication | Can they express uncertainty probabilistically? |

4. **Categorize experts**

   Based on Luu et al. (2024):
   
   | Category | Availability Score | Experience Score | Role |
   |----------|-------------------|------------------|------|
   | Core expert | > 2.5 | > 2.5 | Full participation in workshops |
   | Resource person | ≤ 2.5 | > 2.5 | Consulted as needed |
   | Contributor | Any | ≤ 2.5 | Input during planning only |

### Workshop Size

- **Optimal:** ~20 experts for manageable plenary discussions
- **Too few:** Risk missing important perspectives
- **Too many:** Cumbersome for model building in plenary

### Checklist

- [ ] Knowledge domains identified
- [ ] Expert candidates listed
- [ ] Expertise assessed and scored
- [ ] Experts categorized (core / resource / contributor)
- [ ] Invitations sent with clear expectations
- [ ] Diverse perspectives ensured (discipline, institution, gender)

---

## 4.4 Expert Knowledge Elicitation

### What is Expert Knowledge Elicitation (EKE)?

A systematic process of consulting domain experts to provide:
- Qualitative information about system structure
- Quantitative estimates about uncertain variables
- Relationships between variables
- Probability distributions representing uncertainty

### Why EKE is Necessary

- Empirical data is often limited or unavailable
- Experts can identify causal factors missed by statistical models
- Expert judgment, properly elicited, is more accurate than intuition
- Combines diverse perspectives into coherent models

### EKE Methods

| Method | Description | When to Use |
|--------|-------------|-------------|
| Interviews | One-on-one structured conversations | Initial scoping, sensitive topics |
| Surveys | Standardized questionnaires | Large expert pools, quantitative estimates |
| Workshops | Group facilitation and discussion | Model building, consensus development |
| Structured protocols | Formal elicitation techniques (e.g., SHELF) | High-stakes quantitative estimates |

### Challenges and Pitfalls

⚠️ **Poor probability judgment** — Experts are not trained to express uncertainty as distributions  
⚠️ **Overconfidence** — Experts often underestimate their uncertainty  
⚠️ **Between-expert variation** — Different experts give different estimates  
⚠️ **Targeting errors** — Asking the wrong expert about the wrong topic  
⚠️ **Dominant personalities** — Some voices overpower others in groups  

### Best Practices

From Bolger & Rowe (2015) and Bolger & Wright (2017):

- [ ] **Select the best experts** — Domain-relevant, experienced, available
- [ ] **Train experts in probability** — Calibration training before elicitation (see Phase 5)
- [ ] **Provide well-defined targets** — Clear, unambiguous questions
- [ ] **Offer estimation tools** — Reference points, scales, visual aids
- [ ] **Give feedback** — Help experts learn from their estimates
- [ ] **Combine judgments** — Aggregate multiple experts systematically

### Targeting EKE Appropriately

> "Ask the right people about the right things"

| Expert Type | Ask About | Don't Ask About |
|-------------|-----------|-----------------|
| Nutritionist | Food content, dietary impacts | Tree planting techniques |
| Agronomist | Crop yields, farming practices | Policy implementation |
| Community leader | Social dynamics, local context | Biophysical parameters |
| Economist | Market prices, cost structures | Agronomic details |

### Checklist

- [ ] EKE method selected (interviews, workshops, surveys)
- [ ] Questions/targets clearly defined
- [ ] Experts matched to appropriate questions
- [ ] Elicitation protocol prepared
- [ ] Calibration training scheduled (see Phase 5)
- [ ] Plan for combining expert judgments

---

## 4.5 Generate and Operationalize Impact Pathways

### What is an Impact Pathway?

A graphical and logical representation of:
- How a decision/intervention leads to outcomes
- The intermediate steps and causal mechanisms
- The variables that influence results
- The uncertainties at each stage

**Also called:** Conceptual model, causal model, theory of change

### Building Impact Pathways in Workshops

The collaborative workshop process (Whitney et al., 2018):

1. **Break decision into key questions**
   - In plenary, identify the major components
   - What are the pathways to impact?
   
2. **Small group work**
   - Random/rotating groups work on each question
   - Experts brainstorm variables and relationships
   - Draw graphical models (nodes and edges)
   
3. **Peer review**
   - Groups exchange and critique each other's work
   - Identify missing factors, inconsistencies
   
4. **Plenary synthesis**
   - Combine group outputs into unified model
   - Discuss disagreements, reach consensus
   
5. **Iterate**
   - Repeat until all experts are satisfied
   - Ensure all relevant relationships captured

### Workshop Facilitation Tips

**Overcoming group dynamics challenges:**

- Include mix of extroverted and introverted participants
- Use structured activities to give everyone voice
- Pair individuals for focused problem-solving
- Rotate group composition to prevent dominant coalitions
- Provide written input options for quieter participants

**Addressing cognitive biases:**

- Expert facilitation to promote divergence of opinion
- Anonymous initial estimates before group discussion
- Structured techniques to surface disagreements
- Calibration training to address overconfidence

### Impact Pathway Content

The model should include:

- [ ] **Decision options** — What alternatives are being compared?
- [ ] **Intermediate outcomes** — Steps between action and impact
- [ ] **Final outcomes** — Ultimate objectives (e.g., nutrition, income, sustainability)
- [ ] **Costs** — Resources required for each option
- [ ] **Benefits** — Positive outcomes expected
- [ ] **Risks** — Potential negative outcomes
- [ ] **Influencing factors** — External variables affecting outcomes
- [ ] **Uncertainties** — Parameters that are unknown

### From Conceptual to Quantitative

The impact pathway provides the structure for:
- Identifying variables to estimate
- Defining relationships to quantify
- Building the mathematical model (Phase 6)

### Checklist

- [ ] Workshop planned and experts invited
- [ ] Decision broken into key questions
- [ ] Small group exercises designed
- [ ] Facilitation plan addresses group dynamics
- [ ] Draft impact pathway model developed
- [ ] Model reviewed in plenary
- [ ] All experts confirm completeness
- [ ] Model documented for quantification

---

## Phase 4 Outputs

At the end of this phase, you should have:

1. **Systems Context Document**
   - Literature review synthesis
   - Knowledge gaps identified
   
2. **Stakeholder Map**
   - Categorized stakeholders
   - Engagement strategy
   
3. **Expert Roster**
   - Core experts, resource persons, contributors
   - Domain assignments
   
4. **Impact Pathway Model**
   - Graphical representation of decision → outcomes
   - Variables and relationships identified
   - Ready for quantification

---

## Quality Check

Before proceeding to Calibration (Phase 5):

- [ ] Decision-maker confirms impact pathway captures the decision correctly
- [ ] All experts have reviewed and approved the model structure
- [ ] No major causal factors missing
- [ ] Variables are defined clearly enough to estimate
- [ ] Relationships are logical and defensible

---

## Transition to Calibration

With systems understanding established → train experts to make better estimates (Phase 5: Calibration)
